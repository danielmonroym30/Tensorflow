{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daniel Conrado Monroy Madrid - 16012674"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "dataset=np.load('proyecto_training_data.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La presente implementación busca entrenar los parámetros de una regresión lineal. Para ello, se busca encontrar un valor óptimo de los parámetros del modelo utilizando gradient descent, el cual recibe un valor de learning rate que, en términos sencillos, indica los pasos o la distancia a la dirección. Como se aprendió en el proyecto final del curso y en las primeras semanas del curso de Statiscal Learning, entre menores sean los valores del learning rate, el error tiende a disminuir. En esta tarea se mantiene constante el número de epochs en 100 y se juega con el valor del learning rate para observar cómo se comporta el error del modelo. Dicho de otra manera, en este trabajo se realiza un experimento donde se examina cómo distintos valores de un hiperparámetro inciden en el error, partiendo de la idea de que valores menores de learning rate son favorables para la tarea. Luego de ejecutar cada sesión para cada valor de learning rate, se adjunta la gráfica de comportamiento del error. El resultado que se espera es que un valor pequeño -dentro de las opciones presentes- de learning rate disminuya el error, lo cual se podrá apreciar en la gráfica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_entrenamiento=dataset[0:1168,]\n",
    "x=datos_entrenamiento[:,1]\n",
    "y=datos_entrenamiento[:,0]\n",
    "n=len(x)\n",
    "\n",
    "X=tf.placeholder(tf.float32) \n",
    "Y=tf.placeholder(tf.float32) \n",
    "\n",
    "#Parámetros entrenables del modelo\n",
    "w=tf.Variable(0.0,name='W')\n",
    "b=tf.Variable(0.0,name='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sesión #1 para Learning rate = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=10\n",
    "epochs=100\n",
    "\n",
    "#Modelo de regresión lineal a estimar \n",
    "y_predict=tf.add(tf.multiply(X, w), b)\n",
    "\n",
    "#Función de costo \n",
    "cost=tf.reduce_sum((y_predict-Y)**2)/(2*n)\n",
    "\n",
    "#Gradient descent como optimizador para minimizar la función de costo\n",
    "gd_optimizer=tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "#Summary cost\n",
    "cost_summary=tf.summary.scalar(\"Cost\",cost)    #paso0\n",
    "\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : cost = 2878434500000000.0 W = 11879478.0 b = 1805902.5\n",
      "epoch 10 : cost = inf W = 1.2332538e+33 b = 1.9256778e+32\n",
      "epoch 20 : cost = nan W = nan b = nan\n",
      "epoch 30 : cost = nan W = nan b = nan\n",
      "epoch 40 : cost = nan W = nan b = nan\n",
      "epoch 50 : cost = nan W = nan b = nan\n",
      "epoch 60 : cost = nan W = nan b = nan\n",
      "epoch 70 : cost = nan W = nan b = nan\n",
      "epoch 80 : cost = nan W = nan b = nan\n",
      "epoch 90 : cost = nan W = nan b = nan\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    writer=tf.summary.FileWriter('./graf1',session.graph) #paso 1\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "            session.run(gd_optimizer,feed_dict={X:x,Y:y})\n",
    "            \n",
    "            summary_epoch=session.run(cost_summary,feed_dict={X:x,Y:y}) #paso 2\n",
    "            writer.add_summary(summary_epoch,epoch)  #paso 3 \n",
    "            \n",
    "            if not epoch % 10:\n",
    "                \n",
    "            \n",
    "                c=session.run(cost,feed_dict={X:x,Y:y})\n",
    "                weights=session.run(w)\n",
    "                bias=session.run(b)\n",
    "                print(\"epoch\", epoch, \": cost =\", c, \"W =\", session.run(w), \"b =\", session.run(b))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graf1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sesión #2 para Learning rate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1\n",
    "epochs=100\n",
    "\n",
    "#Modelo de regresión lineal a estimar \n",
    "y_predict=tf.add(tf.multiply(X, w), b)\n",
    "\n",
    "#Función de costo \n",
    "cost=tf.reduce_sum((y_predict-Y)**2)/(2*n)\n",
    "\n",
    "#Gradient descent como optimizador para minimizar la función de costo\n",
    "gd_optimizer=tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "#Summary cost\n",
    "cost_summary=tf.summary.scalar(\"Cost\",cost)    #paso0\n",
    "\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : cost = 27504107000000.0 W = 1187947.8 b = 180590.25\n",
      "epoch 10 : cost = inf W = 9.599841e+21 b = 1.4989796e+21\n",
      "epoch 20 : cost = inf W = 7.9612e+37 b = 1.2431109e+37\n",
      "epoch 30 : cost = nan W = nan b = nan\n",
      "epoch 40 : cost = nan W = nan b = nan\n",
      "epoch 50 : cost = nan W = nan b = nan\n",
      "epoch 60 : cost = nan W = nan b = nan\n",
      "epoch 70 : cost = nan W = nan b = nan\n",
      "epoch 80 : cost = nan W = nan b = nan\n",
      "epoch 90 : cost = nan W = nan b = nan\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    writer=tf.summary.FileWriter('./graf2',session.graph) #paso 1\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "            session.run(gd_optimizer,feed_dict={X:x,Y:y})\n",
    "            \n",
    "            summary_epoch=session.run(cost_summary,feed_dict={X:x,Y:y}) #paso 2\n",
    "            writer.add_summary(summary_epoch,epoch)  #paso 3 \n",
    "            \n",
    "            if not epoch % 10:\n",
    "                \n",
    "            \n",
    "                c=session.run(cost,feed_dict={X:x,Y:y})\n",
    "                weights=session.run(w)\n",
    "                bias=session.run(b)\n",
    "                print(\"epoch\", epoch, \": cost =\", c, \"W =\", session.run(w), \"b =\", session.run(b))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graf2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sesión #3 para Learning rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.1\n",
    "epochs=100\n",
    "\n",
    "#Modelo de regresión lineal a estimar \n",
    "y_predict=tf.add(tf.multiply(X, w), b)\n",
    "\n",
    "#Función de costo \n",
    "cost=tf.reduce_sum((y_predict-Y)**2)/(2*n)\n",
    "\n",
    "#Gradient descent como optimizador para minimizar la función de costo\n",
    "gd_optimizer=tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "#Summary cost\n",
    "cost_summary=tf.summary.scalar(\"Cost\",cost)    #paso0\n",
    "\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : cost = 164311740000.0 W = 118794.77 b = 18059.025\n",
      "epoch 10 : cost = 5.96167e+20 W = 5389484000.0 b = 841542500.0\n",
      "epoch 20 : cost = 2.181536e+30 W = 326018350000000.0 b = 50906654000000.0\n",
      "epoch 30 : cost = inf W = 1.9721562e+19 b = 3.0794385e+18\n",
      "epoch 40 : cost = inf W = 1.1929989e+24 b = 1.8628124e+23\n",
      "epoch 50 : cost = inf W = 7.2167285e+28 b = 1.1268657e+28\n",
      "epoch 60 : cost = inf W = 4.365546e+33 b = 6.816626e+32\n",
      "epoch 70 : cost = nan W = nan b = nan\n",
      "epoch 80 : cost = nan W = nan b = nan\n",
      "epoch 90 : cost = nan W = nan b = nan\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    writer=tf.summary.FileWriter('./graf3',session.graph) #paso 1\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "            session.run(gd_optimizer,feed_dict={X:x,Y:y})\n",
    "            \n",
    "            summary_epoch=session.run(cost_summary,feed_dict={X:x,Y:y}) #paso 2\n",
    "            writer.add_summary(summary_epoch,epoch)  #paso 3 \n",
    "            \n",
    "            if not epoch % 10:\n",
    "                \n",
    "            \n",
    "                c=session.run(cost,feed_dict={X:x,Y:y})\n",
    "                weights=session.run(w)\n",
    "                bias=session.run(b)\n",
    "                print(\"epoch\", epoch, \": cost =\", c, \"W =\", session.run(w), \"b =\", session.run(b))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graf3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sesión #4 para Learning rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.01\n",
    "epochs=100\n",
    "\n",
    "#Modelo de regresión lineal a estimar \n",
    "y_predict=tf.add(tf.multiply(X, w), b)\n",
    "\n",
    "#Función de costo \n",
    "cost=tf.reduce_sum((y_predict-Y)**2)/(2*n)\n",
    "\n",
    "#Gradient descent como optimizador para minimizar la función de costo\n",
    "gd_optimizer=tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "#Summary cost\n",
    "cost_summary=tf.summary.scalar(\"Cost\",cost)    #paso0\n",
    "\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : cost = 7864250000.0 W = 11879.478 b = 1805.9025\n",
      "epoch 10 : cost = 1392429800.0 W = 29602.256 b = 4084.1912\n",
      "epoch 20 : cost = 1389887000.0 W = 29781.879 b = 3625.4927\n",
      "epoch 30 : cost = 1387596400.0 W = 29856.354 b = 3152.6755\n",
      "epoch 40 : cost = 1385327200.0 W = 29929.852 b = 2681.9941\n",
      "epoch 50 : cost = 1383079800.0 W = 30003.0 b = 2213.5369\n",
      "epoch 60 : cost = 1380853900.0 W = 30075.803 b = 1747.2924\n",
      "epoch 70 : cost = 1378648400.0 W = 30148.26 b = 1283.2504\n",
      "epoch 80 : cost = 1376463700.0 W = 30220.377 b = 821.4014\n",
      "epoch 90 : cost = 1374300000.0 W = 30292.152 b = 361.73456\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    writer=tf.summary.FileWriter('./graf4',session.graph) #paso 1\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "            session.run(gd_optimizer,feed_dict={X:x,Y:y})\n",
    "            \n",
    "            summary_epoch=session.run(cost_summary,feed_dict={X:x,Y:y}) #paso 2\n",
    "            writer.add_summary(summary_epoch,epoch)  #paso 3 \n",
    "            \n",
    "            if not epoch % 10:\n",
    "                \n",
    "            \n",
    "                c=session.run(cost,feed_dict={X:x,Y:y})\n",
    "                weights=session.run(w)\n",
    "                bias=session.run(b)\n",
    "                print(\"epoch\", epoch, \": cost =\", c, \"W =\", session.run(w), \"b =\", session.run(b))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graf4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sesión #5 para Learning Rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "epochs=100\n",
    "\n",
    "#Modelo de regresión lineal a estimar \n",
    "y_predict=tf.add(tf.multiply(X, w), b)\n",
    "\n",
    "#Función de costo \n",
    "cost=tf.reduce_sum((y_predict-Y)**2)/(2*n)\n",
    "\n",
    "#Gradient descent como optimizador para minimizar la función de costo\n",
    "gd_optimizer=tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "#Summary cost\n",
    "cost_summary=tf.summary.scalar(\"Cost\",cost)    #paso0\n",
    "\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : cost = 17994813000.0 W = 1187.9478 b = 180.59026\n",
      "epoch 10 : cost = 8720706000.0 W = 10741.556 b = 1623.3298\n",
      "epoch 20 : cost = 4627585000.0 W = 17090.812 b = 2565.745\n",
      "epoch 30 : cost = 2821014500.0 W = 21311.322 b = 3175.788\n",
      "epoch 40 : cost = 2023580400.0 W = 24117.639 b = 3565.0334\n",
      "epoch 50 : cost = 1671514400.0 W = 25984.463 b = 3807.6033\n",
      "epoch 60 : cost = 1516005500.0 W = 27227.148 b = 3952.74\n",
      "epoch 70 : cost = 1447243600.0 W = 28055.201 b = 4033.1565\n",
      "epoch 80 : cost = 1416766700.0 W = 28607.799 b = 4070.585\n",
      "epoch 90 : cost = 1403185800.0 W = 28977.408 b = 4079.4622\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    writer=tf.summary.FileWriter('./graf5',session.graph) #paso 1\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "            session.run(gd_optimizer,feed_dict={X:x,Y:y})\n",
    "            \n",
    "            summary_epoch=session.run(cost_summary,feed_dict={X:x,Y:y}) #paso 2\n",
    "            writer.add_summary(summary_epoch,epoch)  #paso 3 \n",
    "            \n",
    "            if not epoch % 10:\n",
    "                \n",
    "            \n",
    "                c=session.run(cost,feed_dict={X:x,Y:y})\n",
    "                weights=session.run(w)\n",
    "                bias=session.run(b)\n",
    "                print(\"epoch\", epoch, \": cost =\", c, \"W =\", session.run(w), \"b =\", session.run(b))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graf5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sesión #6 para Learning Rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "epochs=100\n",
    "\n",
    "#Modelo de regresión lineal a estimar \n",
    "y_predict=tf.add(tf.multiply(X, w), b)\n",
    "\n",
    "#Función de costo \n",
    "cost=tf.reduce_sum((y_predict-Y)**2)/(2*n)\n",
    "\n",
    "#Gradient descent como optimizador para minimizar la función de costo\n",
    "gd_optimizer=tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "#Summary cost\n",
    "cost_summary=tf.summary.scalar(\"Cost\",cost)    #paso0\n",
    "\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : cost = 19265626000.0 W = 118.79477 b = 18.059025\n",
      "epoch 10 : cost = 17886583000.0 W = 1280.8885 b = 194.61244\n",
      "epoch 20 : cost = 16613941000.0 W = 2397.2747 b = 364.02902\n",
      "epoch 30 : cost = 15439513000.0 W = 3469.752 b = 526.5896\n",
      "epoch 40 : cost = 14355704000.0 W = 4500.0483 b = 682.564\n",
      "epoch 50 : cost = 13355531000.0 W = 5489.824 b = 832.21155\n",
      "epoch 60 : cost = 12432530000.0 W = 6440.675 b = 975.78125\n",
      "epoch 70 : cost = 11580758000.0 W = 7354.132 b = 1113.5121\n",
      "epoch 80 : cost = 10794706000.0 W = 8231.666 b = 1245.6342\n",
      "epoch 90 : cost = 10069310000.0 W = 9074.692 b = 1372.368\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    writer=tf.summary.FileWriter('./graf6',session.graph) #paso 1\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "            session.run(gd_optimizer,feed_dict={X:x,Y:y})\n",
    "            \n",
    "            summary_epoch=session.run(cost_summary,feed_dict={X:x,Y:y}) #paso 2\n",
    "            writer.add_summary(summary_epoch,epoch)  #paso 3 \n",
    "            \n",
    "            if not epoch % 10:\n",
    "                \n",
    "            \n",
    "                c=session.run(cost,feed_dict={X:x,Y:y})\n",
    "                weights=session.run(w)\n",
    "                bias=session.run(b)\n",
    "                print(\"epoch\", epoch, \": cost =\", c, \"W =\", session.run(w), \"b =\", session.run(b))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graf6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusión: como se puede observar en las gráficas, el error se acerca a cero y se mantiene constante cuando el learning rate adquiere un valor de 0.001, por lo que se puede confirmar la idea propuesta al principio de que un learning rate cada vez menor ayuda a minimizar el error (en este caso, se pasó de 10 a 0.001). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
